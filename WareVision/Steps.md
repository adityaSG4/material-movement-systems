VISION for Tech-A-Thon using OpenCV, Algorithms, PyQt

Our Coding Roadmap (Step-by-Step):

    Environment Setup: We'll begin by setting up the required environment with Python, OpenCV, PyTorch (for YOLOv8), PyQt, and other libraries.

    Mobile Camera Input: Create code to acquire video from your mobile camera.

    YOLOv8 Integration: Set up the object detection model and integrate it with the video feed.

    Object Data Extraction: Extract the coordinates, orientation, and ArucoID of detected objects from the AI Model output.

    Path Planning (Basic): Develop a basic path planning algorithm (e.g., A*) using a simplified grid system.

    Basic PyQt UI: Create a basic UI to display the video feed, detected boxes, and the generated path.

    Integration and Testing: Combine all parts and perform an end-to-end test.

    Refinement and Optimization: Go back and improve each section of the system and improve performance and user experience.

Step 1: Environment Setup

    Action: I'll provide a checklist of libraries to install and steps to take to set up a robust environment.

    Your Task: Follow the checklist to set up the environment.

    Expected outcome: A fully setup environment with all the necessary software and libraries.

Step 2: Mobile Camera Input

    Action: I'll provide a Python code snippet using OpenCV to receive video from an IP camera.

    Your Task:

        Test the code by running it and making sure that it can get video from your mobile camera.

        Test this out on your Linux machine and make sure that everything is working fine.

    Expected Outcome: A video stream running successfully in an OpenCV window.

Step 3: YOLOv8 Integration

    Action: I'll provide a Python code snippet for using a pre-trained YOLOv8 model with OpenCV to perform real-time object detection.

    Your Task:

        Integrate the YOLOv8 model into your system and use it to detect all of the objects.

        Fine tune your code to detect the boxes, and other obstacles as accurately as possible.

    Expected Outcome: You will see bounding boxes around objects in your video feed, with class labels, and confidence scores.

Step 4: Object Data Extraction

    Action: I'll give a Python code to extract data from the YOLOv8 output which will include coordinates, orientation, and other information, about the objects detected.

    Your Task: Refine this code so that it extracts all the required data.

    Expected outcome: All of the object data will be correctly extracted, and will be ready to use by your path planning system.

Step 5: Path Planning (Basic)

    Action: I'll provide a basic implementation of the A* algorithm in Python, using a simplified grid system.

    Your Task: Create a grid based system from the sensor data. Integrate the A* algorithm, and test it using some sample data.

    Expected Outcome: An algorithm that is able to generate a path from a start point to an end point while avoiding obstacles.

Step 6: Basic PyQt UI

    Action: I'll give a Python code for a simple PyQt UI that can display the video feed and the path, and show the output from all of the modules.

    Your Task: Use the code to build the PyQt UI. Integrate it with your existing code.

    Expected Outcome: You should have a window that shows the live video, highlights objects with bounding boxes, and displays the path generated.

Step 7: Integration and Testing

    Action: I'll provide instructions to integrate all of the different parts together and test them end to end.

    Your Task: Run all of your modules together and fix any errors that might exist and make sure the whole system is running correctly.

    Expected Outcome: A fully functional end to end system.

Step 8: Refinement and Optimization

    Action: We'll create an action plan for iterative refinement to improve speed, accuracy and reliability, and user experience.

    Your Task: Go back and modify your code based on the action plan.

    Expected Outcome: A stable, accurate, efficient and user friendly system.